# -*- coding: utf-8 -*-
"""Assignment3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XNIMOQz5RJ1HK_yX-Wl9aki5tFiHvEQ9
"""

import matplotlib.pyplot as plt
import numpy as np

from sklearn import datasets, svm
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

"""# Binary Linear Classifier"""

iris = datasets.load_iris()
X = iris.data[:100, :2]
Y = iris.target[:100]

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.8, random_state=0)

logisticRegr = LogisticRegression()

logisticRegr.fit(x_train, y_train)
#predictions = logisticRegr.predict(x_test[:,:2])

x_axis_min = x_train[:,0].min()-1
x_axis_max = x_train[:,0].max()+1
y_axis_min = x_train[:,1].min()-1
y_axis_max = x_train[:,1].max()+1

XX, YY = np.meshgrid(np.arange(x_axis_min, x_axis_max, step = 0.01), np.arange(y_axis_min, y_axis_max, step = 0.01))
Z = logisticRegr.predict(np.c_[XX.ravel(), YY.ravel()])
Z = Z.reshape(XX.shape)

plt.figure(1, figsize=(5,4))
plt.pcolormesh(XX, YY, Z, cmap=plt.cm.Paired)

plt.scatter(x_train[:,0], x_train[:,1], c = y_train, edgecolors='k', cmap=plt.cm.Paired)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')

plt.xlim(XX.min(), XX.max())
plt.ylim(YY.min(), YY.max())

plt.show()

logisticRegr.score(x_test, y_test)

"""# Linear SVM Classifier"""

iris = datasets.load_iris()
X = iris.data[:100, :2]
Y = iris.target[:100]

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.8, random_state=0)

linearSVM = svm.SVC(kernel="linear", C=100)

linearSVM.fit(x_train, y_train)
#predictions = linearSVM.predict(x_test[:,:2])

## FOR PLOTTING DATA ON SCATTER PLOT ##
x_axis_min = x_train[:,0].min()-1
x_axis_max = x_train[:,0].max()+1
y_axis_min = x_train[:,1].min()-1
y_axis_max = x_train[:,1].max()+1

XX, YY = np.meshgrid(np.arange(x_axis_min, x_axis_max, step = 0.01), np.arange(y_axis_min, y_axis_max, step = 0.01))
Z = linearSVM.predict(np.c_[XX.ravel(), YY.ravel()])
Z = Z.reshape(XX.shape)

plt.figure(1, figsize=(5,4))
plt.pcolormesh(XX, YY, Z, cmap=plt.cm.Paired)

plt.scatter(x_train[:,0], x_train[:,1], c = y_train, edgecolors='k', cmap=plt.cm.Paired)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')

plt.xlim(XX.min(), XX.max())
plt.ylim(YY.min(), YY.max())


## FOR DECISION BOUNDARY AND MARGINS ##
sum = 0
for i in linearSVM.coef_[0]:
  sum += np.power(i,2)

d = 1 / np.power(sum, 0.5)

slope = -(linearSVM.coef_[0][0]/linearSVM.coef_[0][1])

offset = d*np.power(1+np.power(slope,2), 0.5)

bias = -linearSVM.intercept_[0]/linearSVM.coef_[0][1]

# Decision Boundary #
x_line = np.linspace(x_axis_min, x_axis_max, 50)
y_line = slope * x_line + bias

plt.plot(x_line, y_line, 'k')

# Margins #
x_line = np.linspace(x_axis_min, x_axis_max, 50)
y_line = slope * x_line + bias + offset

plt.plot(x_line, y_line, 'k--')

x_line = np.linspace(x_axis_min, x_axis_max, 50)
y_line = slope * x_line + bias - offset

plt.plot(x_line, y_line, 'k--')


## FOR CIRCLING SUPPORT VECTORS ##
support_vects1 = linearSVM.support_vectors_[:,0]
support_vects2 = linearSVM.support_vectors_[:,1]
plt.scatter(support_vects1, support_vects2, marker = 'd')

# I Have Used sklearn and svm's given support_vectors function #
# But, from the Duality Theorem and the KKT condition, we can 
# identify the support vectors as it defines them to be ALWAYS on the margin
# and that non-support vectors do not have a role in defining the weight and bias#

plt.show()

linearSVM.score(x_test, y_test)

"""# Q6"""

print("slope:", slope)
print("bias:", bias)
print("offset:", offset)

"""The Value of the Margin is 

x2 = x1 - 2.3 Â± 0.3

because the 

slope of decision boundary is 0.999

bias of decision boundary is -2.298

and the offset for each margin is 0.299

# Q7

Vector w^T in 

(w^T)(x) +b = 0 

is always orthogonal to the decision boundary.

Note: w^T & x are vectors (cannot show in Colab)

# Linear SVM Classifier with test_size = 0.4
"""

iris = datasets.load_iris()
X = iris.data[:100, :2]
Y = iris.target[:100]

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=0)

linearSVM = svm.SVC(kernel="linear", C=100)

linearSVM.fit(x_train, y_train)
#predictions = linearSVM.predict(x_test[:,:2])

## FOR PLOTTING DATA ON SCATTER PLOT ##
x_axis_min = x_train[:,0].min()-1
x_axis_max = x_train[:,0].max()+1
y_axis_min = x_train[:,1].min()-1
y_axis_max = x_train[:,1].max()+1

XX, YY = np.meshgrid(np.arange(x_axis_min, x_axis_max, step = 0.01), np.arange(y_axis_min, y_axis_max, step = 0.01))
Z = linearSVM.predict(np.c_[XX.ravel(), YY.ravel()])
Z = Z.reshape(XX.shape)

plt.figure(1, figsize=(5,4))
plt.pcolormesh(XX, YY, Z, cmap=plt.cm.Paired)

plt.scatter(x_train[:,0], x_train[:,1], c = y_train, edgecolors='k', cmap=plt.cm.Paired)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')

plt.xlim(XX.min(), XX.max())
plt.ylim(YY.min(), YY.max())


## FOR DECISION BOUNDARY AND MARGINS ##
sum = 0
for i in linearSVM.coef_[0]:
  sum += np.power(i,2)

d = 1 / np.power(sum, 0.5)

slope = -(linearSVM.coef_[0][0]/linearSVM.coef_[0][1])

offset = d*np.power(1+np.power(slope,2), 0.5)

bias = -linearSVM.intercept_[0]/linearSVM.coef_[0][1]

# Decision Boundary #
x_line = np.linspace(x_axis_min, x_axis_max, 50)
y_line = slope * x_line + bias

plt.plot(x_line, y_line, 'k')

# Margins #
x_line = np.linspace(x_axis_min, x_axis_max, 50)
y_line = slope * x_line + bias + offset

plt.plot(x_line, y_line, 'k--')

x_line = np.linspace(x_axis_min, x_axis_max, 50)
y_line = slope * x_line + bias - offset

plt.plot(x_line, y_line, 'k--')


## FOR CIRCLING SUPPORT VECTORS ##
support_vects1 = linearSVM.support_vectors_[:,0]
support_vects2 = linearSVM.support_vectors_[:,1]
plt.scatter(support_vects1, support_vects2, marker = 'd')

# I Have Used sklearn and svm's given support_vectors function #
# But, from the Duality Theorem and the KKT condition, we can 
# identify the support vectors as it defines them to be ALWAYS on the margin
# and that non-support vectors do not have a role in defining the weight and bias#

plt.show()

linearSVM.score(x_test, y_test)

"""# Q8

The Decision Boundary has changed, but the Test Accuracy still stayed the same.

The Decision Boundary has changed because there are more (new) training data points available (in test_size = 0.4 than test_size = 0.8) that can act as the Support Vectors when training SVM.
In other words, there are other points that did not exist previously with less training points, which can act as new supporting vectors that "pushes" the Margins closer and thus change the Decision Boundary.

The Test Accuracy did not change as all the points in the test data set were all within their Decision Boundary. Regardless of the training size, the Decision Boundary was preciese enough to contain all the test data points in their correct side.

# Q9

As seen in the two plots (Top: Binary Linear Classifier / Bottom: SVM), the two classifiers do NOT have the same decision boundaries.

# SVM Classifier with 150 Entries
"""

iris = datasets.load_iris()
X = iris.data[:, :2]
Y = iris.target[:]

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=0)

nonlinearSVM = svm.SVC(kernel="poly", C=100)

nonlinearSVM.fit(x_train, y_train)
#predictions = nonlinearSVM.predict(x_test[:,:2])

## FOR PLOTTING DATA ON SCATTER PLOT ##
x_axis_min = x_train[:,0].min()-1
x_axis_max = x_train[:,0].max()+1
y_axis_min = x_train[:,1].min()-1
y_axis_max = x_train[:,1].max()+1

XX, YY = np.meshgrid(np.arange(x_axis_min, x_axis_max, step = 0.01), np.arange(y_axis_min, y_axis_max, step = 0.01))
Z = nonlinearSVM.predict(np.c_[XX.ravel(), YY.ravel()])
Z = Z.reshape(XX.shape)

plt.figure(1, figsize=(5,4))
plt.pcolormesh(XX, YY, Z, cmap=plt.cm.Paired)

plt.scatter(x_train[:,0], x_train[:,1], c = y_train, edgecolors='k', cmap=plt.cm.Paired)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')

plt.xlim(XX.min(), XX.max())
plt.ylim(YY.min(), YY.max())

plt.show()

"""Here, we can see that now that we are not using Binary outcome data (has 3 possible outcomes 0, 1, 2), the data points are not linearly separable. 

We can deal with this using a Soft-SVM, where the Decision Boundary is non linear. Using a Soft-SVM rather than a Hard-SVM can set the Decision Boundary to be more precise to divide the data points into 3 or more sections that are not linearly separable.
"""