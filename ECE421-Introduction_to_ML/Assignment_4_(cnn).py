# -*- coding: utf-8 -*-
"""Assignment 4 (CNN)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZicsGqKGdD4bHXKBzCyv7OkzvRnNWha_

**Assignment**: Designing and Tuning a Convolutional Neural Network (CNN)

**Assignment Description**: There are four parts to this assignment

1.   Building a CNN
2.   Training and Tuning a CNN
3.   Trying Out a New Dataset
4.   Open-Ended Exploration

You will be largely guided through the first two parts. The third and fourth part are discussion based questions. 

**Before the experiment, make sure that you have GPU enabled. This setting can be found under *Tools --> Settings***
"""

#Install Objax
!pip --quiet install  objax
import objax

import tensorflow as tf 
import tensorflow_datasets as tfds
import numpy as np
import jax.numpy as jn
import random 
import matplotlib.pyplot as plt

"""##**Part 1. Building a CNN** 

Before we build our CNN model, let's first import a dataset. For our experiment, we load the CIFAR10 dataset from Tensorflow's dataset repository. The CIFAR10 dataset consists of 60,000 32x32 colour images in 10 classes, with 6000 images per class. The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks.

After loading the dataset, we split the dataset into training, validation and test set. The dataset is originally stored as 50,000 training examples and 10,000 test examples. Instead, we will combine them together and make our own split.

Do not change split ratio for now.
"""

#.load_data() by default returns a split between training and test set. 
# We then adjust the training set into a format that can be accepted by our CNN
(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.cifar10.load_data()
X_train = X_train.transpose(0, 3, 1, 2) / 255.0
Y_train = Y_train.flatten()
X_test = X_test.transpose(0, 3, 1, 2) / 255.0
Y_test = Y_test.flatten()

np.random.seed(1)
# To create a validation set, we first concate the original splitted dataset into a single dataset 
# then randomly shuffle the images and labels in the same way (seed = 1)
X_data = np.concatenate([X_train, X_test], axis = 0)
Y_data = np.concatenate([Y_train, Y_test], axis = 0)

N = np.arange(len(X_data))
np.random.shuffle(N)
X_data = X_data[N]
Y_data = Y_data[N]

#Next, we partition the randomly shuffled dataset into training, validation and testset according a ratio
train_ratio = 0.80
valid_ratio = 0.1
n_train = int(len(X_data) * train_ratio)
n_valid = int(len(X_data) * valid_ratio)

X_train, X_valid, X_test = X_data[:n_train], X_data[n_train:n_train+n_valid], X_data[n_train+n_valid:]
Y_train, Y_valid, Y_test = Y_data[:n_train], Y_data[n_train:n_train+n_valid], Y_data[n_train+n_valid:]

print(n_train)
print(n_valid)

"""
Next we will construct a **Base Model**, which in our case is a small CNN."""

class ConvNet(objax.Module):
  def __init__(self, number_of_channels = 3, number_of_classes = 10):
    self.conv_1 = objax.nn.Sequential([objax.nn.Conv2D(number_of_channels, 16, 2), objax.functional.relu])
    self.conv_2 = objax.nn.Sequential([objax.nn.Conv2D(16, 32, 2), objax.functional.relu])
    self.linear = objax.nn.Linear(32, number_of_classes)

  def __call__(self, x):
    x = objax.functional.max_pool_2d(self.conv_1(x), 2, 2)
    x = self.conv_2(x)
  
    x = x.mean((2,3)) #<--- global average pooling 
    x = self.linear(x)
    return x

#The following line creates the CNN
model = ConvNet()
#You can examine the architecture of our CNN by calling model.vars()

model.vars()

"""Before we train our conv net, let's try to better understand concepts of convolution filter and linear layer. In the following, you will take the first very image of the training set, create a simple convolution routine, and show that our own routine matches what Objax returns. 


"""

#Let's plot the first image in the training set.
plt.imshow(X_train[0].transpose(1,2,0))

"""Next, we will pass our image through Objax's convolution routine. Carefully examine the following code and try to understand the dimension of the filter weights and the output. """

# We append the first image with a batch size of 1 so it can be fed into a convolution layer
my_image = np.expand_dims(X_train[0], 0)

#Consider a very simple CNN filter with stride = 1 and no padding ('VALID').
Conv2d = objax.nn.Conv2D(nin = 3, nout = 2, k = 1, strides = 1, padding = 'VALID', use_bias = False)

filter_weights = Conv2d.w.value #This is the initial weight of the filter, which we gradually update when training, we ignore bias for now

print("Filter weights:", filter_weights)
print("Conv output:", Conv2d(my_image))
print("Conv output shape:", np.shape(Conv2d(my_image)))

print("DIMENSIONS Filter:", np.shape(filter_weights))
print("DIMENSIONS Input:", np.shape(my_image))
print("DIMENSIONS Output:", np.shape(Conv2d(my_image)))

"""**In the cells below, you will create your own convolution routine that takes in the image and the initial weights used by Objax's own convolution routine (Conv2d.w.value) and show that your convolution routine returns the same value than Objax's.**

A simple implementation only requires 4 FOR loops. You may wish to draw inspiration from https://objax.readthedocs.io/en/latest/objax/nn.html?highlight=objax.nn.Conv2D#objax.nn.Conv2D
"""

#Solution to the above problem

def my_conv_net(my_image, initial_filter_weights):
  # "PUT YOUR CODE HERE"

  # DIMENSION Input: (N, C_in, H_in, W_in) = (1, 3, 32, 32) 
  N = 1 #Batch Size
  C_in = 3 #Channel
  H_in = 32 #Height
  W_in = 32 #Width
  # DIMENSION Filter: (k, k, C_in, C_out) = (1, 1, 3, 2)
  k = 1 #Kernel Size
  C_out = 2 #Channel
  # DIMENSION Output (N, C_out, H_out, W_out) = (1, 2, 32, 32)
  H_out = H_in - k + 1 #32
  W_out = W_in - k + 1 #32

  my_conv_output = np.zeros([N, C_out, H_out, W_out])

  for n in range(N): #0
    for c in range(C_out): #0-1
      for h in range(H_out): #0-31
        for w in range(W_out): #0-31
          my_conv_output[n, c, h, w] = np.sum(my_image[n, :, h, w] * initial_filter_weights[:,:,:,c])

  return my_conv_output

#Showing that my Convolution Routine returns the same value as Objax's
my_conv_output = my_conv_net(my_image, filter_weights)

#print("Conv output from my_routine", my_conv_output)
#print("Conv output from obj_routine:", Conv2d(my_image))
print("Conv output DIFFERENCE:", Conv2d(my_image) - my_conv_output) #Shows DIFF is almost equal to 0

"""The outputs of last convolution layer is typically rearranged so it can be fed into a linear layer. Check that calling .mean((2,3)) rearranges the output of your convolution routine by examining the shape of the output. (Not graded) Think about alternative ways of rearranging the output from the convolution layer. """

#Check that .mean((2,3)) rearranges your image
my_conv_output.mean((2,3))

np.shape(my_conv_output)

"""Take your rearranged output and feed it into a linear layer of appropriate size. Here is an example:

```
Linear_Layer = objax.nn.Linear(N, 1)
Y = Linear_Layer(X)
```
Next, extract the weights and bias of the linear layer using 
```
Linear_Layer.w.value
Linear_Layer.b.value
```
**Using these values, write one line of code that manually implements the linear layer. Show that it provides the same value as Objax's own linear layer.** 


"""

#PUT YOUR CODE HERE

#X := .mean((2,3))
X = my_conv_output.mean((2,3))

#Objax's Linear Layer
Linear_Layer = objax.nn.Linear(2, 1) #objax.nn.Linear(nin, nout) #Linear(Channels_in, Channels_out)
Y_obj = Linear_Layer(X)

#Extract Weights and Bias
W = Linear_Layer.w.value
B = Linear_Layer.b.value

#My Manual Linear Layer
Y_man = np.dot(X, W) + B

print(Y_obj)
print(Y_man)
same = round(Y_obj, 7) == round(Y_man, 7) #Rounded up to 7 Decimals for Comparison
print(same)

"""You have now completed Part 1 of the assignment. Good job!

##**Part 2. Training and Tuning a CNN**

The following starter code trains the neural network in Part 1. However, the optimizer and batch sampling routine are left for you to implement. Complete the lines that says #PUT YOUR CODE HERE#

Afterwards, train the model, and observe the training/validation loss and accuracy plots. You should observe that the validation accuracy is low and stagnates after a few epochs.
"""

#Define loss function as averaged value of of cross entropies
def loss_function(x, labels):
    logit = model(x)
    return objax.functional.loss.cross_entropy_logits_sparse(logit, labels).mean()

#Define a prediction function
predict = objax.Jit(lambda x: objax.functional.softmax(model(x)), model.vars()) 

#Create an object that can be used to calculate the gradient and value of loss_function
gv= objax.GradValues(loss_function, model.vars())

#Create an object that can be used to provide trainable variables in the model
tv = objax.ModuleList(objax.TrainRef(x) for x in model.vars().subset(objax.TrainVar))

#Training routine
def train_op(x, y, learning_rate):
    lr = learning_rate
    gradient, loss_value = gv(x, y)   # calculate gradient and loss value "backprop"
    #next we update the trainable parameter using SGD and similar procedure
    for grad, params in zip(gradient, tv.vars()):
      ####################
      #PUT YOUR CODE HERE#
      params.assign(params.value - grad * lr)
      ####################                      
    return loss_value                      # return loss value

#make train_op (much) faster using JIT compilation
train_op = objax.Jit(train_op, gv.vars() + tv.vars())

def train(EPOCHS = 20, BATCH = 32, LEARNING_RATE = 9e-4):
  avg_train_loss_epoch = []
  avg_val_loss_epoch = []
  train_acc_epoch = []
  val_acc_epoch = []

  for epoch in range(EPOCHS):
      avg_train_loss = 0 # (averaged) training loss per batch
      avg_val_loss =  0  # (averaged) validation loss per batch
      train_acc = 0      # training accuracy per batch
      val_acc = 0        # validation accuracy per batch

      # shuffle the examples prior to training to remove correlation 
      train_indices = np.arange(len(X_train)) 
      np.random.shuffle(train_indices)
      for it in range(0, X_train.shape[0], BATCH):
          #PUT YOUR CODE HERE#
          batch = train_indices[it : it+BATCH] 

          avg_train_loss += float(train_op(X_train[batch], Y_train[batch], LEARNING_RATE)[0]) * len(batch)
          train_prediction = predict(X_train[batch]).argmax(1)
          train_acc += (np.array(train_prediction).flatten() == Y_train[batch]).sum()
      train_acc_epoch.append(train_acc/X_train.shape[0])
      avg_train_loss_epoch.append(avg_train_loss/X_train.shape[0])

      # run validation
      val_indices = np.arange(len(X_valid)) 
      np.random.shuffle(val_indices)    
      for it in range(0, X_valid.shape[0], BATCH):
          #PUT YOUR CODE HERE#
          batch = val_indices[it : it+BATCH] 
          
          avg_val_loss += float(loss_function(X_valid[batch], Y_valid[batch])) * len(batch)
          val_prediction = predict(X_valid[batch]).argmax(1)
          val_acc += (np.array(val_prediction).flatten() == Y_valid[batch]).sum()
      val_acc_epoch.append(val_acc/X_valid.shape[0])
      avg_val_loss_epoch.append(avg_val_loss/X_valid.shape[0])

      print('Epoch %04d  Training Loss %.2f Validation Loss %.2f Training Accuracy %.2f Validation Accuracy %.2f' % (epoch + 1, avg_train_loss/X_train.shape[0], avg_val_loss/X_valid.shape[0], 100*train_acc/X_train.shape[0], 100*val_acc/X_valid.shape[0]))
  
  #Plot training loss
  plt.title("Train vs Validation Loss")
  plt.plot(avg_train_loss_epoch, label="Train")
  plt.plot(avg_val_loss_epoch, label="Validation")
  plt.xlabel("Epoch")
  plt.ylabel("Loss")
  plt.legend(loc='best')
  plt.show()

  plt.title("Train vs Validation Accuracy")
  plt.plot(train_acc_epoch, label="Train")
  plt.plot(val_acc_epoch, label="Validation")
  plt.xlabel("Epoch")
  plt.ylabel("Accuracy (%)")
  plt.legend(loc='best')
  plt.show()

train()

"""Follow the assignment handout for questions to be answered in this part of the assignment."""

#PUT YOUR CODE HERE (Use as many cells as you want)

"""My 4 Hyperparameters:

H = {1. Batch size, 2. Learning rate, 3. Number of outputs of Conv layer1, 4. Number of Conv layers}
"""

### Hyperparameters Set #1 ###
### Hyperparameters Set #1 ###
### Hyperparameters Set #1 ###

# H = {1. Batch size, 2. Learning rate, 3. Number of outputs of Conv layer1, 4. Number of Conv layers}
# H1 = {32, 0.001, 16, 2}
class ConvNet1(objax.Module):
  def __init__(self, number_of_channels = 3, number_of_classes = 10):
    self.conv_1 = objax.nn.Sequential([objax.nn.Conv2D(number_of_channels, 16, 2), objax.functional.relu]) ## Number of outputs of conv layer1 = 16 ##
    self.conv_2 = objax.nn.Sequential([objax.nn.Conv2D(16, 32, 2), objax.functional.relu])
    self.linear = objax.nn.Linear(32, number_of_classes)

  def __call__(self, x):
    x = objax.functional.max_pool_2d(self.conv_1(x), 2, 2)
    x = self.conv_2(x)
  
    x = x.mean((2,3)) #<--- global average pooling 
    x = self.linear(x)
    return x

#The following line creates the CNN
model1 = ConvNet1()
#You can examine the architecture of our CNN by calling model.vars()

#Define loss function as averaged value of of cross entropies
def loss_function1(x, labels):
    logit = model1(x)
    return objax.functional.loss.cross_entropy_logits_sparse(logit, labels).mean()

#Define a prediction function
predict1 = objax.Jit(lambda x: objax.functional.softmax(model1(x)), model1.vars())

#Create an object that can be used to calculate the gradient and value of loss_function
gv1 = objax.GradValues(loss_function1, model1.vars())

#Create an object that can be used to provide trainable variables in the model
tv1 = objax.ModuleList(objax.TrainRef(x) for x in model1.vars().subset(objax.TrainVar))

#Training routine
def train_op1(x, y, learning_rate):
    lr = learning_rate
    gradient, loss_value = gv1(x, y)   # calculate gradient and loss value "backprop"
    #next we update the trainable parameter using SGD and similar procedure
    for grad, params in zip(gradient, tv1.vars()):
      ####################
      #PUT YOUR CODE HERE#
      params.assign(params.value - grad * lr)
      ####################                      
    return loss_value                      # return loss value

#make train_op (much) faster using JIT compilation
train_op1 = objax.Jit(train_op1, gv1.vars() + tv1.vars())

def train1(EPOCHS = 20, BATCH = 32, LEARNING_RATE = 0.001): ## Batch Size = 32 | Learning Rate = 0.001 ##
  avg_train_loss_epoch = []
  avg_val_loss_epoch = []
  train_acc_epoch = []
  val_acc_epoch = []

  for epoch in range(EPOCHS):
      avg_train_loss = 0 # (averaged) training loss per batch
      avg_val_loss =  0  # (averaged) validation loss per batch
      train_acc = 0      # training accuracy per batch
      val_acc = 0        # validation accuracy per batch

      # shuffle the examples prior to training to remove correlation 
      train_indices = np.arange(len(X_train)) 
      np.random.shuffle(train_indices)
      for it in range(0, X_train.shape[0], BATCH):
          #PUT YOUR CODE HERE#
          batch = train_indices[it : it+BATCH] 

          avg_train_loss += float(train_op1(X_train[batch], Y_train[batch], LEARNING_RATE)[0]) * len(batch)
          train_prediction = predict1(X_train[batch]).argmax(1)
          train_acc += (np.array(train_prediction).flatten() == Y_train[batch]).sum()
      train_acc_epoch.append(train_acc/X_train.shape[0])
      avg_train_loss_epoch.append(avg_train_loss/X_train.shape[0])

      # run validation
      val_indices = np.arange(len(X_valid)) 
      np.random.shuffle(val_indices)    
      for it in range(0, X_valid.shape[0], BATCH):
          #PUT YOUR CODE HERE#
          batch = val_indices[it : it+BATCH] 
          
          avg_val_loss += float(loss_function1(X_valid[batch], Y_valid[batch])) * len(batch)
          val_prediction = predict1(X_valid[batch]).argmax(1)
          val_acc += (np.array(val_prediction).flatten() == Y_valid[batch]).sum()
      val_acc_epoch.append(val_acc/X_valid.shape[0])
      avg_val_loss_epoch.append(avg_val_loss/X_valid.shape[0])

      print('Epoch %04d  Training Loss %.2f Validation Loss %.2f Training Accuracy %.2f Validation Accuracy %.2f' % (epoch + 1, avg_train_loss/X_train.shape[0], avg_val_loss/X_valid.shape[0], 100*train_acc/X_train.shape[0], 100*val_acc/X_valid.shape[0]))
  
  #Plot training loss
  plt.title("Train vs Validation Loss")
  plt.plot(avg_train_loss_epoch, label="Train")
  plt.plot(avg_val_loss_epoch, label="Validation")
  plt.xlabel("Epoch")
  plt.ylabel("Loss")
  plt.legend(loc='best')
  plt.show()

  plt.title("Train vs Validation Accuracy")
  plt.plot(train_acc_epoch, label="Train")
  plt.plot(val_acc_epoch, label="Validation")
  plt.xlabel("Epoch")
  plt.ylabel("Accuracy (%)")
  plt.legend(loc='best')
  plt.show()

train1()

### Hyperparameters Set #2 ###
### Hyperparameters Set #2 ###
### Hyperparameters Set #2 ###

# H = {1. Batch size, 2. Learning rate, 3. Number of outputs of Conv layer1, 4. Number of Conv layers}
# H2 = {64, 0.0001, 32, 3}
class ConvNet2(objax.Module):
  def __init__(self, number_of_channels = 3, number_of_classes = 10):
    self.conv_1 = objax.nn.Sequential([objax.nn.Conv2D(number_of_channels, 32, 2), objax.functional.relu]) ## Number of outputs of conv layer1 = 32 ##
    self.conv_2 = objax.nn.Sequential([objax.nn.Conv2D(32, 32, 2), objax.functional.relu])  # Number of Input of conv layer2 changes to 32
    self.conv_3 = objax.nn.Sequential([objax.nn.Conv2D(32, 32, 2), objax.functional.relu])  ## Conv layer 3 ##
    self.linear = objax.nn.Linear(32, number_of_classes)

  def __call__(self, x):
    x = objax.functional.max_pool_2d(self.conv_1(x), 2, 2)
    x = self.conv_2(x)
    x = self.conv_3(x)
  
    x = x.mean((2,3)) #<--- global average pooling 
    x = self.linear(x)
    return x

#The following line creates the CNN
model2 = ConvNet2()
#You can examine the architecture of our CNN by calling model.vars()

#Define loss function as averaged value of of cross entropies
def loss_function2(x, labels):
    logit = model2(x)
    return objax.functional.loss.cross_entropy_logits_sparse(logit, labels).mean()


#Define a prediction function
predict2 = objax.Jit(lambda x: objax.functional.softmax(model2(x)), model2.vars()) 

#Create an object that can be used to calculate the gradient and value of loss_function
gv2 = objax.GradValues(loss_function2, model2.vars())

#Create an object that can be used to provide trainable variables in the model
tv2 = objax.ModuleList(objax.TrainRef(x) for x in model2.vars().subset(objax.TrainVar))

#Training routine
def train_op2(x, y, learning_rate):
    lr = learning_rate
    gradient, loss_value = gv2(x, y)   # calculate gradient and loss value "backprop"
    #next we update the trainable parameter using SGD and similar procedure
    for grad, params in zip(gradient, tv2.vars()):
      ####################
      #PUT YOUR CODE HERE#
      params.assign(params.value - grad * lr)
      ####################                      
    return loss_value                      # return loss value

#make train_op (much) faster using JIT compilation
train_op2 = objax.Jit(train_op2, gv2.vars() + tv2.vars())

def train2(EPOCHS = 20, BATCH = 64, LEARNING_RATE = 0.0001): ## Batch Size = 64 | Learning Rate = 0.0001 ##
  avg_train_loss_epoch = []
  avg_val_loss_epoch = []
  train_acc_epoch = []
  val_acc_epoch = []

  for epoch in range(EPOCHS):
      avg_train_loss = 0 # (averaged) training loss per batch
      avg_val_loss =  0  # (averaged) validation loss per batch
      train_acc = 0      # training accuracy per batch
      val_acc = 0        # validation accuracy per batch

      # shuffle the examples prior to training to remove correlation 
      train_indices = np.arange(len(X_train)) 
      np.random.shuffle(train_indices)
      for it in range(0, X_train.shape[0], BATCH):
          #PUT YOUR CODE HERE#
          batch = train_indices[it : it+BATCH] 

          avg_train_loss += float(train_op2(X_train[batch], Y_train[batch], LEARNING_RATE)[0]) * len(batch)
          train_prediction = predict2(X_train[batch]).argmax(1)
          train_acc += (np.array(train_prediction).flatten() == Y_train[batch]).sum()
      train_acc_epoch.append(train_acc/X_train.shape[0])
      avg_train_loss_epoch.append(avg_train_loss/X_train.shape[0])

      # run validation
      val_indices = np.arange(len(X_valid)) 
      np.random.shuffle(val_indices)    
      for it in range(0, X_valid.shape[0], BATCH):
          #PUT YOUR CODE HERE#
          batch = val_indices[it : it+BATCH] 
          
          avg_val_loss += float(loss_function2(X_valid[batch], Y_valid[batch])) * len(batch)
          val_prediction = predict2(X_valid[batch]).argmax(1)
          val_acc += (np.array(val_prediction).flatten() == Y_valid[batch]).sum()
      val_acc_epoch.append(val_acc/X_valid.shape[0])
      avg_val_loss_epoch.append(avg_val_loss/X_valid.shape[0])

      print('Epoch %04d  Training Loss %.2f Validation Loss %.2f Training Accuracy %.2f Validation Accuracy %.2f' % (epoch + 1, avg_train_loss/X_train.shape[0], avg_val_loss/X_valid.shape[0], 100*train_acc/X_train.shape[0], 100*val_acc/X_valid.shape[0]))
  
  #Plot training loss
  plt.title("Train vs Validation Loss")
  plt.plot(avg_train_loss_epoch, label="Train")
  plt.plot(avg_val_loss_epoch, label="Validation")
  plt.xlabel("Epoch")
  plt.ylabel("Loss")
  plt.legend(loc='best')
  plt.show()

  plt.title("Train vs Validation Accuracy")
  plt.plot(train_acc_epoch, label="Train")
  plt.plot(val_acc_epoch, label="Validation")
  plt.xlabel("Epoch")
  plt.ylabel("Accuracy (%)")
  plt.legend(loc='best')
  plt.show()

train2()

## EVALUATION ON TEST SET ##

def test_ACC_Base(EPOCHS = 20, BATCH = 32, LEARNING_RATE = 9e-4):
  #avg_train_loss_epoch = []
  #avg_val_loss_epoch = []
  #train_acc_epoch = []
  #val_acc_epoch = []
  test_acc_epoch = []

  for epoch in range(EPOCHS):
      #avg_train_loss = 0 # (averaged) training loss per batch
      #avg_val_loss =  0  # (averaged) validation loss per batch
      #train_acc = 0      # training accuracy per batch
      #val_acc = 0        # validation accuracy per batch
      test_acc = 0

      # shuffle the examples prior to training to remove correlation 
      test_indices = np.arange(len(X_test)) 
      np.random.shuffle(test_indices)
      for it in range(0, X_test.shape[0], BATCH):
          #PUT YOUR CODE HERE#
          batch = test_indices[it : it+BATCH] 
          #avg_train_loss += float(train_op(X_train[batch], Y_train[batch], LEARNING_RATE)[0]) * len(batch)
          test_prediction = predict(X_test[batch]).argmax(1)
          test_acc += (np.array(test_prediction).flatten() == Y_test[batch]).sum()
      test_acc_epoch.append(test_acc/X_test.shape[0])
      #avg_train_loss_epoch.append(avg_train_loss/X_train.shape[0])

  print('Test Accuracy %.2f' % (100*test_acc/X_test.shape[0]))
  
test_ACC_Base()

def test_ACC_M1(EPOCHS = 20, BATCH = 32, LEARNING_RATE = 9e-4):
  #avg_train_loss_epoch = []
  #avg_val_loss_epoch = []
  #train_acc_epoch = []
  #val_acc_epoch = []
  test_acc_epoch = []

  for epoch in range(EPOCHS):
      #avg_train_loss = 0 # (averaged) training loss per batch
      #avg_val_loss =  0  # (averaged) validation loss per batch
      #train_acc = 0      # training accuracy per batch
      #val_acc = 0        # validation accuracy per batch
      test_acc = 0

      # shuffle the examples prior to training to remove correlation 
      test_indices = np.arange(len(X_test)) 
      np.random.shuffle(test_indices)
      for it in range(0, X_test.shape[0], BATCH):
          #PUT YOUR CODE HERE#
          batch = test_indices[it : it+BATCH] 
          #avg_train_loss += float(train_op(X_train[batch], Y_train[batch], LEARNING_RATE)[0]) * len(batch)
          test_prediction = predict1(X_test[batch]).argmax(1)
          test_acc += (np.array(test_prediction).flatten() == Y_test[batch]).sum()
      test_acc_epoch.append(test_acc/X_test.shape[0])
      #avg_train_loss_epoch.append(avg_train_loss/X_train.shape[0])

  print('Test Accuracy %.2f' % (100*test_acc/X_test.shape[0]))
  
test_ACC_M1()

def test_ACC_M2(EPOCHS = 20, BATCH = 32, LEARNING_RATE = 9e-4):
  #avg_train_loss_epoch = []
  #avg_val_loss_epoch = []
  #train_acc_epoch = []
  #val_acc_epoch = []
  test_acc_epoch = []

  for epoch in range(EPOCHS):
      #avg_train_loss = 0 # (averaged) training loss per batch
      #avg_val_loss =  0  # (averaged) validation loss per batch
      #train_acc = 0      # training accuracy per batch
      #val_acc = 0        # validation accuracy per batch
      test_acc = 0

      # shuffle the examples prior to training to remove correlation 
      test_indices = np.arange(len(X_test)) 
      np.random.shuffle(test_indices)
      for it in range(0, X_test.shape[0], BATCH):
          #PUT YOUR CODE HERE#
          batch = test_indices[it : it+BATCH] 
          #avg_train_loss += float(train_op(X_train[batch], Y_train[batch], LEARNING_RATE)[0]) * len(batch)
          test_prediction = predict2(X_test[batch]).argmax(1)
          test_acc += (np.array(test_prediction).flatten() == Y_test[batch]).sum()
      test_acc_epoch.append(test_acc/X_test.shape[0])
      #avg_train_loss_epoch.append(avg_train_loss/X_train.shape[0])

  print('Test Accuracy %.2f' % (100*test_acc/X_test.shape[0]))
  
test_ACC_M2()

"""You have now completed Part 2 of the assignment. Good job!

##**Part 3. Trying Out a New Dataset**

See the handout for instructions.
"""

### I HAVE USED cifar100 Dataset ###

#.load_data() by default returns a split between training and test set. 
# We then adjust the training set into a format that can be accepted by our CNN
(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.cifar100.load_data()
X_train = X_train.transpose(0, 3, 1, 2) / 255.0
Y_train = Y_train.flatten()
X_test = X_test.transpose(0, 3, 1, 2) / 255.0
Y_test = Y_test.flatten()

np.random.seed(1)
# To create a validation set, we first concate the original splitted dataset into a single dataset 
# then randomly shuffle the images and labels in the same way (seed = 1)
X_data = np.concatenate([X_train, X_test], axis = 0)
Y_data = np.concatenate([Y_train, Y_test], axis = 0)

N = np.arange(len(X_data))
np.random.shuffle(N)
X_data = X_data[N]
Y_data = Y_data[N]

#Next, we partition the randomly shuffled dataset into training, validation and testset according a ratio
train_ratio = 0.80
valid_ratio = 0.1
n_train = int(len(X_data) * train_ratio)
n_valid = int(len(X_data) * valid_ratio)

X_train, X_valid, X_test = X_data[:n_train], X_data[n_train:n_train+n_valid], X_data[n_train+n_valid:]
Y_train, Y_valid, Y_test = Y_data[:n_train], Y_data[n_train:n_train+n_valid], Y_data[n_train+n_valid:]

print(n_train)
print(n_valid)

###################################
###################################
###          BASE MODEL         ### 
### H_Base = {32, 0.001, 16, 2} ###
###################################
###################################

# H = {1. Batch size, 2. Learning rate, 3. Number of outputs of Conv layer1, 4. Number of Conv layers}
# H_Base = {32, 0.001, 16, 2}
class ConvNet_BASE(objax.Module):
  def __init__(self, number_of_channels = 3, number_of_classes = 100):
    self.conv_1 = objax.nn.Sequential([objax.nn.Conv2D(number_of_channels, 16, 2), objax.functional.relu])
    self.conv_2 = objax.nn.Sequential([objax.nn.Conv2D(16, 32, 2), objax.functional.relu])
    self.linear = objax.nn.Linear(32, number_of_classes)

  def __call__(self, x):
    x = objax.functional.max_pool_2d(self.conv_1(x), 2, 2)
    x = self.conv_2(x)
  
    x = x.mean((2,3)) #<--- global average pooling 
    x = self.linear(x)
    return x

#The following line creates the CNN
model_BASE = ConvNet_BASE()
#You can examine the architecture of our CNN by calling model.vars()

#Define loss function as averaged value of of cross entropies
def loss_function_BASE(x, labels):
    logit = model_BASE(x)
    return objax.functional.loss.cross_entropy_logits_sparse(logit, labels).mean()

#Define a prediction function
predict_BASE = objax.Jit(lambda x: objax.functional.softmax(model_BASE(x)), model_BASE.vars()) 

#Create an object that can be used to calculate the gradient and value of loss_function
gv_BASE = objax.GradValues(loss_function_BASE, model_BASE.vars())

#Create an object that can be used to provide trainable variables in the model
tv_BASE = objax.ModuleList(objax.TrainRef(x) for x in model_BASE.vars().subset(objax.TrainVar))

#Training routine
def train_op_BASE(x, y, learning_rate):
    lr = learning_rate
    gradient, loss_value = gv_BASE(x, y)   # calculate gradient and loss value "backprop"
    #next we update the trainable parameter using SGD and similar procedure
    for grad, params in zip(gradient, tv_BASE.vars()):
      ####################
      #PUT YOUR CODE HERE#
      params.assign(params.value - grad * lr)
      ####################                      
    return loss_value                      # return loss value

#make train_op (much) faster using JIT compilation
train_op_BASE = objax.Jit(train_op_BASE, gv_BASE.vars() + tv_BASE.vars())

def train_BASE(EPOCHS = 20, BATCH = 32, LEARNING_RATE = 0.001):
  avg_train_loss_epoch = []
  avg_val_loss_epoch = []
  train_acc_epoch = []
  val_acc_epoch = []

  for epoch in range(EPOCHS):
      avg_train_loss = 0 # (averaged) training loss per batch
      avg_val_loss =  0  # (averaged) validation loss per batch
      train_acc = 0      # training accuracy per batch
      val_acc = 0        # validation accuracy per batch

      # shuffle the examples prior to training to remove correlation 
      train_indices = np.arange(len(X_train)) 
      np.random.shuffle(train_indices)
      for it in range(0, X_train.shape[0], BATCH):
          #PUT YOUR CODE HERE#
          batch = train_indices[it : it+BATCH] 

          avg_train_loss += float(train_op_BASE(X_train[batch], Y_train[batch], LEARNING_RATE)[0]) * len(batch)
          train_prediction = predict_BASE(X_train[batch]).argmax(1)
          train_acc += (np.array(train_prediction).flatten() == Y_train[batch]).sum()
      train_acc_epoch.append(train_acc/X_train.shape[0])
      avg_train_loss_epoch.append(avg_train_loss/X_train.shape[0])

      # run validation
      val_indices = np.arange(len(X_valid)) 
      np.random.shuffle(val_indices)    
      for it in range(0, X_valid.shape[0], BATCH):
          #PUT YOUR CODE HERE#
          batch = val_indices[it : it+BATCH] 
          
          avg_val_loss += float(loss_function_BASE(X_valid[batch], Y_valid[batch])) * len(batch)
          val_prediction = predict_BASE(X_valid[batch]).argmax(1)
          val_acc += (np.array(val_prediction).flatten() == Y_valid[batch]).sum()
      val_acc_epoch.append(val_acc/X_valid.shape[0])
      avg_val_loss_epoch.append(avg_val_loss/X_valid.shape[0])

      print('Epoch %04d  Training Loss %.2f Validation Loss %.2f Training Accuracy %.2f Validation Accuracy %.2f' % (epoch + 1, avg_train_loss/X_train.shape[0], avg_val_loss/X_valid.shape[0], 100*train_acc/X_train.shape[0], 100*val_acc/X_valid.shape[0]))
  
  #Plot training loss
  plt.title("Train vs Validation Loss")
  plt.plot(avg_train_loss_epoch, label="Train")
  plt.plot(avg_val_loss_epoch, label="Validation")
  plt.xlabel("Epoch")
  plt.ylabel("Loss")
  plt.legend(loc='best')
  plt.show()

  plt.title("Train vs Validation Accuracy")
  plt.plot(train_acc_epoch, label="Train")
  plt.plot(val_acc_epoch, label="Validation")
  plt.xlabel("Epoch")
  plt.ylabel("Accuracy (%)")
  plt.legend(loc='best')
  plt.show()

train_BASE()

###################################
###################################
###          MODEL #5           ### 
### H_M5 = {32, 0.05, 64, 4} ###
###################################
###################################

# H = {1. Batch size, 2. Learning rate, 3. Number of outputs of Conv layer1, 4. Number of Conv layers}
# H_M5 = {32, 0.05, 64, 4}
class ConvNet_M5(objax.Module):
  def __init__(self, number_of_channels = 3, number_of_classes = 100):
    self.conv_1 = objax.nn.Sequential([objax.nn.Conv2D(number_of_channels, 64, 2), objax.functional.relu])
    self.conv_2 = objax.nn.Sequential([objax.nn.Conv2D(64, 128, 2), objax.functional.relu])
    self.conv_3 = objax.nn.Sequential([objax.nn.Conv2D(128, 216, 2), objax.functional.relu])
    self.conv_4 = objax.nn.Sequential([objax.nn.Conv2D(216, 512, 2), objax.functional.relu])


    self.linear = objax.nn.Linear(512, number_of_classes)

  def __call__(self, x):
    x = objax.functional.max_pool_2d(self.conv_1(x), 2, 2)
    x = objax.functional.max_pool_2d(self.conv_2(x), 2, 2)
    x = objax.functional.max_pool_2d(self.conv_3(x), 2, 2)
    x = self.conv_4(x)
  
    x = x.mean((2,3)) #<--- global average pooling 
    x = self.linear(x)
    return x

#The following line creates the CNN
model_M5 = ConvNet_M5()
#You can examine the architecture of our CNN by calling model.vars()

#Define loss function as averaged value of of cross entropies
def loss_function_M5(x, labels):
    logit = model_M5(x)
    return objax.functional.loss.cross_entropy_logits_sparse(logit, labels).mean()

#Define a prediction function
predict_M5 = objax.Jit(lambda x: objax.functional.softmax(model_M5(x)), model_M5.vars()) 

#Create an object that can be used to calculate the gradient and value of loss_function
gv_M5 = objax.GradValues(loss_function_M5, model_M5.vars())

#Create an object that can be used to provide trainable variables in the model
tv_M5 = objax.ModuleList(objax.TrainRef(x) for x in model_M5.vars().subset(objax.TrainVar))

#Training routine
def train_op_M5(x, y, learning_rate):
    lr = learning_rate
    gradient, loss_value = gv_M5(x, y)   # calculate gradient and loss value "backprop"
    #next we update the trainable parameter using SGD and similar procedure
    for grad, params in zip(gradient, tv_M5.vars()):
      ####################
      #PUT YOUR CODE HERE#
      params.assign(params.value - grad * lr)
      ####################                      
    return loss_value                      # return loss value

#make train_op (much) faster using JIT compilation
train_op_M1 = objax.Jit(train_op_M5, gv_M5.vars() + tv_M5.vars())

def train_M5(EPOCHS = 20, BATCH = 32, LEARNING_RATE = 0.05):
  avg_train_loss_epoch = []
  avg_val_loss_epoch = []
  train_acc_epoch = []
  val_acc_epoch = []

  for epoch in range(EPOCHS):
      avg_train_loss = 0 # (averaged) training loss per batch
      avg_val_loss =  0  # (averaged) validation loss per batch
      train_acc = 0      # training accuracy per batch
      val_acc = 0        # validation accuracy per batch

      # shuffle the examples prior to training to remove correlation 
      train_indices = np.arange(len(X_train)) 
      np.random.shuffle(train_indices)
      for it in range(0, X_train.shape[0], BATCH):
          #PUT YOUR CODE HERE#
          batch = train_indices[it : it+BATCH] 

          avg_train_loss += float(train_op_M1(X_train[batch], Y_train[batch], LEARNING_RATE)[0]) * len(batch)
          train_prediction = predict_M5(X_train[batch]).argmax(1)
          train_acc += (np.array(train_prediction).flatten() == Y_train[batch]).sum()
      train_acc_epoch.append(train_acc/X_train.shape[0])
      avg_train_loss_epoch.append(avg_train_loss/X_train.shape[0])

      # run validation
      val_indices = np.arange(len(X_valid)) 
      np.random.shuffle(val_indices)    
      for it in range(0, X_valid.shape[0], BATCH):
          #PUT YOUR CODE HERE#
          batch = val_indices[it : it+BATCH] 
          
          avg_val_loss += float(loss_function_M5(X_valid[batch], Y_valid[batch])) * len(batch)
          val_prediction = predict_M5(X_valid[batch]).argmax(1)
          val_acc += (np.array(val_prediction).flatten() == Y_valid[batch]).sum()
      val_acc_epoch.append(val_acc/X_valid.shape[0])
      avg_val_loss_epoch.append(avg_val_loss/X_valid.shape[0])

      print('Epoch %04d  Training Loss %.2f Validation Loss %.2f Training Accuracy %.2f Validation Accuracy %.2f' % (epoch + 1, avg_train_loss/X_train.shape[0], avg_val_loss/X_valid.shape[0], 100*train_acc/X_train.shape[0], 100*val_acc/X_valid.shape[0]))
  
  #Plot training loss
  plt.title("Train vs Validation Loss")
  plt.plot(avg_train_loss_epoch, label="Train")
  plt.plot(avg_val_loss_epoch, label="Validation")
  plt.xlabel("Epoch")
  plt.ylabel("Loss")
  plt.legend(loc='best')
  plt.show()

  plt.title("Train vs Validation Accuracy")
  plt.plot(train_acc_epoch, label="Train")
  plt.plot(val_acc_epoch, label="Validation")
  plt.xlabel("Epoch")
  plt.ylabel("Accuracy (%)")
  plt.legend(loc='best')
  plt.show()

train_M5()

def test_ACC_M5(EPOCHS = 20, BATCH = 32, LEARNING_RATE = 0.05):
  #avg_train_loss_epoch = []
  #avg_val_loss_epoch = []
  #train_acc_epoch = []
  #val_acc_epoch = []
  test_acc_epoch = []

  for epoch in range(EPOCHS):
      #avg_train_loss = 0 # (averaged) training loss per batch
      #avg_val_loss =  0  # (averaged) validation loss per batch
      #train_acc = 0      # training accuracy per batch
      #val_acc = 0        # validation accuracy per batch
      test_acc = 0

      # shuffle the examples prior to training to remove correlation 
      test_indices = np.arange(len(X_test)) 
      np.random.shuffle(test_indices)
      for it in range(0, X_test.shape[0], BATCH):
          #PUT YOUR CODE HERE#
          batch = test_indices[it : it+BATCH] 
          #avg_train_loss += float(train_op(X_train[batch], Y_train[batch], LEARNING_RATE)[0]) * len(batch)
          test_prediction = predict_M5(X_test[batch]).argmax(1)
          test_acc += (np.array(test_prediction).flatten() == Y_test[batch]).sum()
      test_acc_epoch.append(test_acc/X_test.shape[0])
      #avg_train_loss_epoch.append(avg_train_loss/X_train.shape[0])

  print('Test Accuracy %.2f' % (100*test_acc/X_test.shape[0]))
  
test_ACC_M5()

"""##**Problem 4. Open-Ended Exploration**

See the handout for instructions.
"""